{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da91b4e3-9958-447c-8c26-a5aabc749d1c",
   "metadata": {},
   "source": [
    "# SISTEMA RAG\n",
    "Este código permite detectar y justificar la aparición de TTPs de DISARM en una noticia de entrada, basando su razonamiento en información obtenida de fuentes externas.\n",
    "\n",
    "En primer lugar, se deben haber instalado las dependencias necesarias (requirements.txt).\n",
    "\n",
    "Es necesario disponer de un token de HuggingFace y otro de la API de Event Registry\n",
    "\n",
    "El código se ha estructurado en 3 fases: Indexing, Retrieval y Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef8ec7-8e6a-4bf7-a387-27b9b95bf299",
   "metadata": {},
   "source": [
    "## 1. INDEXING\n",
    "En esta fase se extraerán las palabras clave de la noticia de entrada para obtener noticias relacionadas a partir de la API de Event Registry. Los artículos obtenidos se fragmentarán en splits para posteriormente ser representados en vectores (embeddings) y ser almacenados en una base de datos vectorial.\n",
    "\n",
    "### 1.1. Extractor de keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75379c-6f0a-4731-9194-efab001c9513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Descargar stopwords solo la primera vez\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Función que devuelve True si a y b son suficientemente similares (es decir, si ratio > threshold)\n",
    "def similar(a, b, threshold=0.7):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > threshold\n",
    "\n",
    "    \n",
    "    \n",
    "# Creamos un Extractor de keywords con el modelo KeyBERT\n",
    "class KeywordExtractor:\n",
    "    def __init__(self):\n",
    "        self.kw_model = KeyBERT()\n",
    "        self.stop_words = set(stopwords.words('english')) # Todas las noticiaas de entrada están en inglés\n",
    "        self.punctuation = set(string.punctuation)\n",
    "        self.min_word_length = 3\n",
    "\n",
    "        \n",
    "    # Limpia y normaliza una keyword\n",
    "    def clean_keyword(self, kw):\n",
    "        kw = kw.lower().strip()\n",
    "        kw = ''.join([c for c in kw if c not in self.punctuation])\n",
    "        return kw if (kw and kw not in self.stop_words \n",
    "                     and len(kw) >= self.min_word_length) else None\n",
    "\n",
    "        \n",
    "    # Extrae top_n keywords usando KeyBERT + filtrado tradicional\n",
    "    def extract_keywords(self, text, top_n=5):\n",
    "        # Extracción inicial con KeyBERT\n",
    "        keywords = self.kw_model.extract_keywords(\n",
    "            text,\n",
    "            keyphrase_ngram_range=(1, 2),  # Permite palabras sueltas y bigramas\n",
    "            stop_words='english',\n",
    "            top_n=top_n*3  # Extraemos más para luego filtrar\n",
    "        )\n",
    "        \n",
    "        # Procesamiento y limpieza\n",
    "        cleaned = []\n",
    "        for kw, score in keywords:\n",
    "            kw_clean = self.clean_keyword(kw)\n",
    "            if kw_clean:\n",
    "                # Separar bigramas para mejor filtrado\n",
    "                cleaned.extend(kw_clean.split())\n",
    "        \n",
    "        # Frecuencia de términos y selección\n",
    "        word_counts = Counter(cleaned)\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: (-x[1], -len(x[0])))\n",
    "        \n",
    "        # Selección diversificada\n",
    "        selected = []\n",
    "        for word, count in sorted_words:\n",
    "            if not any(similar(word, s) for s in selected): # Evitar palabras demasiado similares\n",
    "                selected.append(word)\n",
    "                if len(selected) >= top_n:\n",
    "                    break\n",
    "        \n",
    "        return selected[:top_n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071a244-9cc2-4f9a-86a5-d4fc14a5154b",
   "metadata": {},
   "source": [
    "### 1.2. Extracción de noticias parecidas\n",
    "Es necesario registrarse en la web newsapi de Event Registry y obtener una API key para poder hacer llamadas al servicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670472f7-7ac2-459c-be34-6be26efb8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventregistry import EventRegistry, QueryArticlesIter, QueryItems, ReturnInfo, ArticleInfoFlags\n",
    "import requests\n",
    "import re\n",
    "\n",
    "er_key = # API key de Event Registry\n",
    "er = EventRegistry(apiKey=er_key)\n",
    "\n",
    "# Extraer título y cuerpo asumiendo formato \"TITLE: ... BODY: ...\"\n",
    "def extraer_titulo_y_cuerpo(texto):\n",
    "    titulo_match = re.search(r\"TITLE:\\s*(.*?)\\s*BODY:\", texto, re.DOTALL | re.IGNORECASE)\n",
    "    cuerpo_match = re.search(r\"BODY:\\s*(.*)\", texto, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    titulo = titulo_match.group(1).strip() if titulo_match else \"\"\n",
    "    cuerpo = cuerpo_match.group(1).strip() if cuerpo_match else \"\"\n",
    "    return titulo, cuerpo\n",
    "        \n",
    "# Para no obtener de la API la misma noticia que la de entrada\n",
    "def mismo_articulo(art, noticia):\n",
    "    titulo_original, cuerpo_original = extraer_titulo_y_cuerpo(noticia)\n",
    "\n",
    "    titulo_art = art[\"title\"].strip()\n",
    "    cuerpo_art = art.get(\"body\", \"\").strip()\n",
    "\n",
    "    titulo_sim = similar(titulo_art, titulo_original, threshold=0.7)\n",
    "    cuerpo_sim = similar(cuerpo_art[:500], cuerpo_original[:500], threshold=0.7)\n",
    "\n",
    "    es_mismo = titulo_sim or cuerpo_sim\n",
    "    return es_mismo\n",
    "\n",
    "# Llamada a Event Registry con las keywords\n",
    "def buscar_noticias_parecidas(texto, max_noticias=100, lang=\"eng\"):\n",
    "    # Extraemos las palabras clave\n",
    "    extractor = KeywordExtractor()\n",
    "    keywords = extractor.extract_keywords(texto)\n",
    "    print(f\"Keywords seleccionadas: {keywords}\")\n",
    "    \n",
    "    if not keywords:\n",
    "        return []\n",
    "\n",
    "    # Construir parámetros de la query\n",
    "    query_params = {\n",
    "        \"keywords\": QueryItems.OR(keywords),\n",
    "        \"lang\": lang        \n",
    "    }\n",
    "    \n",
    "    # Construir query \n",
    "    q = QueryArticlesIter(**query_params)\n",
    "    #print(q)\n",
    "    ret_info = ReturnInfo(articleInfo=ArticleInfoFlags(bodyLen=-1))\n",
    "\n",
    "    # Llamada a la API\n",
    "    try:\n",
    "        articles = list(q.execQuery(er, returnInfo=ret_info, maxItems=max_noticias))\n",
    "        filtrados = [art for art in articles if not mismo_articulo(art, texto)]\n",
    "        return filtrados\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la búsqueda: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8c30d-4e7d-40ce-b5c5-bbf5a6e778b3",
   "metadata": {},
   "source": [
    "### 1.3. Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f272e-14ea-4ed9-b852-bf700d89cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "# Definimos el splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, # Tamaño máximo de los fragmentos\n",
    "    chunk_overlap=50) # Para evitar la pérdida de contexto entre los trozos\n",
    "\n",
    "# Convertir un artículo en formato Document para poder ser fragmentado\n",
    "def convertir_a_documento(art):\n",
    "    body = art[\"body\"][:5000] # Hasta 5000 caracteres para no generar demasiados tokens. \n",
    "    # Filtrar los metadatos complejos antes de crear el Document\n",
    "    metadata = {\n",
    "        \"title\"        : art[\"title\"],\n",
    "        \"published\"    : art[\"dateTimePub\"],\n",
    "        \"source_name\"  : art[\"source\"][\"title\"],\n",
    "        \"source_domain\": art[\"source\"][\"uri\"],\n",
    "        \"url\"          : art[\"url\"],\n",
    "        \"authors\"      : \", \".join(a[\"name\"] for a in art[\"authors\"]),\n",
    "        \"event_uri\"    : art.get(\"eventUri\"),\n",
    "        \"sentiment\"    : art.get(\"sentiment\"),\n",
    "        \"body_len\"     : len(art[\"body\"]),\n",
    "    }\n",
    "    doc = Document(\n",
    "        page_content = body,\n",
    "        metadata     = metadata\n",
    "    )\n",
    "    return doc\n",
    "\n",
    "# Fragmentar noticias\n",
    "def hacer_splits(articulos):\n",
    "    documentos = [convertir_a_documento(articulo) for articulo in articulos]\n",
    "    splits = text_splitter.split_documents(documentos)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b509d87f-b4f5-448d-9ee8-446768a4c265",
   "metadata": {},
   "source": [
    "### 1.4. Almacenamiento en la BBDD vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bb2fc-a28e-4a7a-8f79-4d85f7fabf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registro previo en HuggingFace para acceder a los modelos\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcacefa-79fa-4c80-935c-2dee4224bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Cargamos el modelo para vectorizar los fragmentos\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Obtener los splits más relevantes de la base de datos\n",
    "def almacenar_splits(splits):\n",
    "    # Inicializar el cliente de Chroma\n",
    "    client = chromadb.Client(Settings())\n",
    "    collection_name = \"temporal_context\"\n",
    "\n",
    "    # Eliminar si ya existía\n",
    "    try:\n",
    "        client.delete_collection(collection_name)\n",
    "    except Exception:\n",
    "        pass  # No pasa nada si no existía\n",
    "\n",
    "\n",
    "    # Crear una colección con similitud del coseno\n",
    "    collection = client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # Especificar la métrica de distancia\n",
    "    )\n",
    "\n",
    "    # Integrar con LangChain\n",
    "    vectorstore = Chroma(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=embeddings # Para vectorizar los fragmentos\n",
    "    )\n",
    "\n",
    "    # Agregar documentos a la colección\n",
    "    vectorstore.add_documents(splits)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f3e3f-11a4-4a55-9a29-7b3a6cbc27e4",
   "metadata": {},
   "source": [
    "## 2. RETRIEVAL\n",
    "En esta fase se obtienen los fragmentos más relevantes respecto a la noticia de entrada de la base de datos vectorial creada durante el proceso de Indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e2497-0ca9-449c-bece-e634b0fb88d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer de la BBDD los 4 fragmentos más relevantes\n",
    "def obtener_splits_relevantes_bbdd(splits, noticia):\n",
    "    vectorstore = almacenar_splits(splits)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "    docs = retriever.invoke(noticia) #invoke llama a getRelevantDocuments, este último se va a dejar de usar\n",
    "    print(\"Cantidad de splits relevantes:\", len(docs))\n",
    "    context_docs = \"\\n\\n\".join(serialize_doc(d) for d in docs)\n",
    "    \n",
    "    return context_docs\n",
    "\n",
    "\n",
    "def serialize_doc(doc):\n",
    "    meta = doc.metadata\n",
    "    return (\n",
    "        f\"<<TITLE>>\\n\"\n",
    "        f\"{meta['title']}\\n\"\n",
    "        #f\"source={meta['source_name']} ({meta['source_domain']})\\n\"\n",
    "        f\"<<TEXT>>\\n\"\n",
    "        f\"{doc.page_content}\\n\"\n",
    "        f\"<<END>>\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee87a3d-e2ea-4253-b18d-753310bc1f2b",
   "metadata": {},
   "source": [
    "## 3. GENERATION\n",
    "En esta última etapa se carga el LLM y se le pasa una noticia de entrada, a partir de ella se extraen los fragmentos de noticias más relevantes y se cargan las TTPs de DISARM que puede detectar.\n",
    "\n",
    "El sistema devuleve una respuesta en lenguaje natural con las TTPs obtenidas y una justificación de su elección."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530e0d9-7ec7-45a3-a2d1-b6c0d005fb6b",
   "metadata": {},
   "source": [
    "### 3.1. Cargar el LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e39988-112b-4ae2-af8a-cefc5048418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Cargar LLM y tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Función para generar la respuesta\n",
    "def llm_generate(prompt):\n",
    "    # Tokenizar input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device) # return_tensors=\"pt\" porque AutoModelForCausalLM está basado en PyTorch y espera como entrada tensores de PyTorch\n",
    "    \n",
    "    # Generar respuesta\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return result[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eec345-e483-4fb1-b74d-dcc621db6a10",
   "metadata": {},
   "source": [
    "### 3.2. Construir el prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a284b12-0bf0-4322-915e-f0f2d4f1ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Función para cargar las TTPs de DISARM\n",
    "def cargar_ttps(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    bloques = []\n",
    "    for obj in data[\"objects\"]:\n",
    "        if obj[\"type\"] == \"tactic\":\n",
    "            tactic_name = obj[\"name\"]\n",
    "            for tech in obj.get(\"techniques\", []):\n",
    "                id_ = tech[\"id\"]\n",
    "                name = tech[\"name\"]\n",
    "                desc = tech[\"description\"].strip()\n",
    "                bloques.append(f\"Technique: {id_} - {name} (Tactic: {tactic_name})\\nTechnique Description: {desc}\")\n",
    "    return \"\\n\\n\".join(bloques)\n",
    "    \n",
    "\n",
    "# Función para obtener contexto relevante llamando a las funciones de Indexing y Retrieval\n",
    "def obtener_contexto (noticia):\n",
    "    articulos = buscar_noticias_parecidas(noticia)\n",
    "    splits = hacer_splits(articulos)\n",
    "    docs = obtener_splits_relevantes_bbdd(splits, noticia)\n",
    "    return docs\n",
    "\n",
    "    \n",
    "# Definición del prompt\n",
    "def construir_prompt(news_text):\n",
    "    ttps_text = cargar_ttps(\"DISARMfiltrado.json\")\n",
    "    news_context = obtener_contexto(news_text)\n",
    "    \n",
    "    return f\"\"\"\n",
    "        You are an expert in communication, disinformation, and discourse analysis. You will now receive one news article to analyze, a list of known manipulation and disinformation techniques (DISARM TTPs), and a few other news articles with related context.\n",
    "        \n",
    "        Your task is to determine whether the FIRST ARTICLE that you receive contains any patterns of manipulation or disinformation using the CONTEXT ARTICLES as background evidence.\n",
    "\n",
    "        Use the TTPs section below as the reference list of patterns to detect.\n",
    "        \n",
    "       \"ttps_detected\": [string], // TTP names detected e.g. [\"T0066-Degrade Adversary\", \"T0015-Create Hashtags and Search Artefacts\"]\n",
    "       \"key_phrases\": [string], // Phrases or evidence that support your findings\n",
    "       \"justification\": string // A concise explanation of your reasoning\n",
    "       \n",
    "        \n",
    "        If no clear technique is detected, respond:\n",
    "        \n",
    "        \"ttps_detected\": [],\n",
    "        \"key_phrases\": [],\n",
    "        \"justification\": string // A concise explanation of your reasoning.\n",
    "\n",
    "        \n",
    "        FIRST ARTICLE to analyze:\n",
    "        {news_text}\n",
    "\n",
    "        CONTEXT ARTICLES:\n",
    "        {news_context}\n",
    "\n",
    "        TTPs (DISARM framework):\n",
    "        {ttps_text}\n",
    "\n",
    "        RESPONSE:\n",
    "        ===\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d87d297-4af5-476e-84ff-03b7af2a0230",
   "metadata": {},
   "source": [
    "### 3.3. Generación de la respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a171b-f02c-44fe-a38a-c3c5aa2b6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTICIA QUE SE QUIERE ANALIZAR con formato \"TITLE:... BODY:... \n",
    "noticia = \"TITLE: BUSTED: Leaked Documents Prove Trump Took Laundered Money From Russian Bank. BODY: Convincing President Trump to release his tax returns is proving slightly more difficult than we initially anticipated, but that doesn t mean there haven t been any signs of success from taking the longer route. Take, for example, a 98-page document recently released by the United States Office of Government Ethics.The document, available in its entirety here, clearly shows that not only is Donald Trump outright profiting from the presidency, a direct violation of the Emoluments Clause of the United States Constitution but also that he is in debt to several banks, both domestic and foreign.Although shocking news, none of this particularly comes as a surprise, more or less just confirms what most of us already suspected. However, it s when you delve into the details that you discover the true significance of the President s possible under-the-table actions.German-based Deutsche Bank was served with $630 million in penalties back in January for a $10 billion Russian money laundering scheme involving it s Moscow and New York branches among others. Deutsche Bank also gave Trump four questionable long-term  loans,  potentially Russian money that s been handed to Trump, the final loan given just before the commencement of the presidential election and used to help fund the Trump International Hotel that opened in Washington DC last year.Obviously, this interaction between Trump and Deutsche Bank should raise more than just a few red flags, at least confirming that there is a possibility that the Russians laundered money to Trump as he began his campaign and just prior to hiring senior advisors with ties to the Russian government.Furthermore, Deutsche Bank is gaining somewhat of a reputation for their shady business dealings as well. Not only were they caught in the Russian laundering scheme, the bank also struck a $7.2 billion deal with the US government last December to settle claims for toxic mortgages they packaged and sold between 2005 and 2007, as well as paying $2.5 billion in April 2015 to settle charges it conspired to manipulate global interest rate benchmarks.It s interesting, yet not at all surprising, how corrupt people and organizations just seem to gravitate toward each other.\"\n",
    "\n",
    "prompt = construir_prompt(noticia)\n",
    "result = llm_generate(prompt)\n",
    "\n",
    "# RESPUESTA DEL LLM\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
